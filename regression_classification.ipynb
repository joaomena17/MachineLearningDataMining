{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start this project by importing the dataset and cleaning the data as we did in the previous project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from sklearn import linear_model, model_selection, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['neighbourhood_group','license','id','host_id','host_name','calculated_host_listings_count','last_review', 'reviews_per_month', 'number_of_reviews_ltm'])    #drop non-desired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the rating and convert to numeric\n",
    "rating_format1 = df['name'].str.extract(r'(\\d+\\.\\d+)')  #rating format \"x.x\"\n",
    "rating_format2 = df['name'].str.extract(r'(\\d+\\.\\d+)\\/')  #rating format \"x.x/\"\n",
    "df['rating'] = pd.to_numeric(rating_format1.fillna(rating_format2)[0], errors='coerce')\n",
    "\n",
    "#extract the num of bed,bedrooms, baths\n",
    "df['bedroom'] = df['name'].str.extract(r'(\\d+)\\s+bedroom')  # Extracts the number of bedrooms\n",
    "df['bed'] = df['name'].str.extract(r'(\\d+)\\s+bed')  # Extracts the number of beds\n",
    "df['bath'] = df['name'].str.extract(r'(\\d+)\\s+bath')  # Extracts the number of baths\n",
    "\n",
    "#convert the columns to the appropriate data types (int - since it's num of bed,bedrooms, baths)\n",
    "df['bedroom'] = pd.to_numeric(df['bedroom'], errors='coerce')\n",
    "df['bed'] = pd.to_numeric(df['bed'], errors='coerce')\n",
    "df['bath'] = pd.to_numeric(df['bath'], errors='coerce')\n",
    "\n",
    "#drop 'name' column\n",
    "df.drop(columns=['name'], inplace=True)\n",
    "\n",
    "# drop 'bed' column\n",
    "df = df.drop(columns='bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we're missing values in one of these 9 filtered columns, we remove that row..\n",
    "if df['price'].isna().any():\n",
    "    print(f\"Column has missing values.\")\n",
    "else:\n",
    "    print(f\"Column has no missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0) # swap NaN for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter outliers in price and minimum nights\n",
    "df = df[(df['price'] >= df['price'].quantile(0.01)) & (df['price'] <= df['price'].quantile(0.99))]\n",
    "\n",
    "df = df[(df['minimum_nights'] <= df['minimum_nights'].quantile(0.99))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform log transformation to normalize price and minimum nights since it was right skewed\n",
    "df['price'] = np.log(df['price'])\n",
    "df['minimum_nights'] = np.log(df['minimum_nights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize price\n",
    "df['price'] = (df['price'] - df['price'].mean()) / df['price'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize number of reviews\n",
    "df = df[(df['number_of_reviews'] > 0)]\n",
    "df['number_of_reviews'] = np.log(df['number_of_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize bed\n",
    "df = df[(df['bedroom'] > 0)]\n",
    "df['bedroom'] = np.log(df['bedroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize bath using cubic root\n",
    "df['bath'] = np.cbrt(df['bath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the 'latitude' and 'longitude' columns\n",
    "df['latitude'] = (df['latitude'] - df['latitude'].mean()) / df['latitude'].std()\n",
    "df['longitude'] = (df['longitude'] - df['longitude'].mean()) / df['longitude'].std()\n",
    "\n",
    "# binzatize the 'rating' column with 3 as the treshold\n",
    "df['rating'] = df['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "# standardize bedroom and bath\n",
    "df['bedroom'] = (df['bedroom'] - df['bedroom'].mean()) / df['bedroom'].std()\n",
    "df['bath'] = (df['bath'] - df['bath'].mean()) / df['bath'].std()\n",
    "\n",
    "# binarize minimum nights with the median as the threshold\n",
    "df['minimum_nights'] = df['minimum_nights'].apply(lambda x: 1 if x > df['minimum_nights'].median() else 0)\n",
    "\n",
    "# binarize number of reviews with the median as the threshold\n",
    "df['number_of_reviews'] = df['number_of_reviews'].apply(lambda x: 1 if x > df['number_of_reviews'].median() else 0)\n",
    "\n",
    "# binarize availability_365 with the median as the threshold\n",
    "df['availability_365'] = df['availability_365'].apply(lambda x: 1 if x > df['availability_365'].median() else 0)\n",
    "\n",
    "# perform one-out-of-k encoding on the 'room_type'\n",
    "df = pd.get_dummies(df, columns=['room_type'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().applymap(\"{0:.2f}\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='neighbourhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression part a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "criterion_variables = df[['price', 'latitude', 'longitude', 'bedroom', 'bath']]\n",
    "\n",
    "X = df.drop('price', axis=1)  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Categorical and numerical column names\n",
    "categorical_cols = ['room_type_Entire home/apt', 'room_type_Hotel room', 'room_type_Private room', 'room_type_Shared room']\n",
    "binary_cols = ['number_of_reviews', 'availability_365', 'minimum_nights', 'rating']\n",
    "numeric_cols = ['latitude', 'longitude', 'bedroom', 'bath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of lambda values to test\n",
    "lambdas = np.logspace(-4, 0.2, 50)\n",
    "\n",
    "all_errors = {}\n",
    "\n",
    "generalization_errors = []\n",
    "\n",
    "# Define number of folds for KFold cross-validation\n",
    "n_folds = 10\n",
    "\n",
    "# Create KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_number = 0\n",
    "fold_error_list = []\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X):\n",
    "    fold_number += 1\n",
    "    print(f\"Fold {fold_number} of {n_folds}...\")\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize list to store error for each fold\n",
    "    error_list = []\n",
    "\n",
    "    for lmbda in lambdas:\n",
    "        # Create Ridge regression object\n",
    "        model = Ridge(lmbda)\n",
    "\n",
    "        # Fit the model on the training data for this fold\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the testing data for this fold\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate the mean squared error for this fold\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        \n",
    "        # Append the error to the error list\n",
    "        error_list.append(mse)\n",
    "\n",
    "    fold_error_list.append(error_list)\n",
    "    \n",
    "\n",
    "    # Calculate the generalization error for this fold\n",
    "    generalization_error = np.mean(error_list)\n",
    "\n",
    "    err_dict = {'best_lambda': lambdas[error_list.index(min(error_list))], 'error': min(error_list)}\n",
    "\n",
    "    all_errors[fold_number] = err_dict\n",
    "\n",
    "\n",
    "    # Append the generalization error to the list\n",
    "    generalization_errors.append(generalization_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for error in all_errors:\n",
    "    print(f\"Fold {error}: {all_errors[error]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = 4\n",
    "\n",
    "print(f\"Minimum Generalization Error: {min(generalization_errors)}\")\n",
    "print(f\"Lambda value with minimum generalization error: {lambdas[generalization_errors.index(min(generalization_errors))]}\")\n",
    "\n",
    "#Average generalisation error vs regularisation parameter (lambda)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lambdas, fold_error_list[best_fold-1], marker='o')\n",
    "plt.xlabel('λ (Regularization Parameter)')\n",
    "plt.ylabel('Average Generalization Error')\n",
    "plt.title('Generalization Error as a Function of λ')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization error curve remains low for a range of lambda values and then increases at the end without exhibiting a clear U-shaped pattern. That suggests that the model is not very sensitive to changes in the regularization parameter (lambda) over the specified range. A lack of a U-shaped curve indicates that the model is robust, meaning that it doesn't seem to overfit (high variance) or underfit (high bias) across the range of lambda values tested. The most optimal lambda value is hard to distiguish, since the one that minimizes the generalization error, but in this case, it may not be distinctly observable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.columns\n",
    "lambda_values = np.logspace(-1, 6, 100)\n",
    "coefficients = []\n",
    "\n",
    "#fitting Ridge regression models for different lambda values and storing coefficients\n",
    "for alpha in lambda_values:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefficients.append(ridge.coef_)\n",
    "\n",
    "#coefficient profile plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(lambda_values, coefficients)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (Regularization Strength)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Ridge Regression Coefficient Profile')\n",
    "plt.legend(feature_names, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression \"penalizes\" the variable coefficients, such that those that are the least effective, \"shrink\" the fastest. In general, as alpha increases, the pentalty for large coefficients also increases. This decreases the absolute values of the coefficients. If certain coefficients flatten out sooner than others, it represents reaching the regularisation state. If the coefficient values remain large, it means that it may be a significant features when making predictions. If certain coefficients flatten out later than others, it means that they may be insignificant features when making predictions. \n",
    "\n",
    "Imagine you have a budget allocated and each coefficient can take some to play a role in the estimation. Naturally those who are more important will take more of the budget. As you increase the lambda, you are decreasing the budget, i.e. penalizing more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will compare three models: the regularized linear regression model from the previous section, an artificial neural network and a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize list to store generalization errors for each lambda value\n",
    "generalization_errors = []\n",
    "\n",
    "# Loop through each fold\n",
    "mean_y_train = np.mean(y_train)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "# compute baseline error for 10 outer folds\n",
    "for train_index, test_index in kf.split(X):\n",
    "    fold += 1\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Initialize list to store error for each fold\n",
    "    error_list = []\n",
    "\n",
    "    # Compute the mean of y_train\n",
    "    y_mean = np.mean(y_train)\n",
    "\n",
    "    # Compute the baseline MSE for this fold\n",
    "    baseline_mse = mean_squared_error(y_test, np.full(len(y_test), y_mean))\n",
    "\n",
    "    print(f\"Baseline MSE for fold {fold}: {baseline_mse}\")\n",
    "\n",
    "# Calculate the average generalization error\n",
    "average_generalization_error = np.mean(generalization_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "generalization_errors = []\n",
    "\n",
    "# Define number of folds for KFold cross-validation\n",
    "n_folds = 10\n",
    "\n",
    "# Create KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "fold_number = 0\n",
    "fold_error_list = []\n",
    "\n",
    "param_distributions = {\n",
    "            'hidden_layer_sizes': [(1,), (10,), (25,), (40,), (50,)]  # Testing different sizes for a single hidden layer\n",
    "        }\n",
    "\n",
    "errors_by_fold = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    \n",
    "    fold_number += 1\n",
    "\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Create MLP regression object\n",
    "    ann = MLPRegressor(hidden_layer_sizes=param_distributions, activation='relu', solver='adam', max_iter=10000, random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(estimator=ann, param_distributions=param_distributions, cv=10, scoring='neg_mean_squared_error')    \n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator from the search\n",
    "    best_ann = search.best_estimator_\n",
    "    hidden_layer_size = best_ann.get_params()['hidden_layer_sizes']\n",
    "\n",
    "    # Fit the best estimator on the training data\n",
    "    best_ann.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    predictions = best_ann.predict(X_fold_test)\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_fold_test, predictions)\n",
    "\n",
    "    print(f\"Current fold: {fold_number},  Optimal hidden layer size: {hidden_layer_size}, Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# List of MSEs for each fold from ANN\n",
    "mse_ann = [\n",
    "    0.4416,\n",
    "    0.3905,\n",
    "    0.4795,\n",
    "    0.4795,\n",
    "    0.4709,\n",
    "    0.4575,\n",
    "    0.4304,\n",
    "    0.4601,\n",
    "    0.4143,\n",
    "    0.4377\n",
    "]  \n",
    "\n",
    "# List of MSEs for each fold from Linear Regression\n",
    "mse_linear = [\n",
    "    0.5462907953225147,\n",
    "    0.546975763341854,\n",
    "    0.5752950136028192,\n",
    "    0.500884931457507,\n",
    "    0.6080982778664757,\n",
    "    0.5343998159373244,\n",
    "    0.5372433882424416,\n",
    "    0.5131378558592264,\n",
    "    0.5268944003529467,\n",
    "    0.5715907993105115\n",
    "]  \n",
    "\n",
    "# List of MSEs for each fold from Baseline\n",
    "mse_baseline = [\n",
    "    1.0160014312259356,\n",
    "    0.9565232234464234,\n",
    "    1.00042283226036,\n",
    "    0.9585350644543394,\n",
    "    1.0328813178777085,\n",
    "    1.0234779533815574,\n",
    "    1.030870624784249,\n",
    "    0.9676963853623705,\n",
    "    1.0058567962898126,\n",
    "    1.0136289464187866\n",
    "]  \n",
    "\n",
    "# Function to calculate the confidence interval\n",
    "def compute_confidence_interval(data1, data2, confidence=0.95):\n",
    "    diff = np.array(data1) - np.array(data2)\n",
    "    mean_diff = np.mean(diff)\n",
    "    sem = stats.sem(diff)\n",
    "    dof = len(diff) - 1\n",
    "    t_crit = np.abs(stats.t.ppf((1 - confidence) / 2, dof))\n",
    "    margin_of_error = t_crit * sem\n",
    "    lower_bound = mean_diff - margin_of_error\n",
    "    upper_bound = mean_diff + margin_of_error\n",
    "    return (lower_bound, upper_bound)\n",
    "\n",
    "# Performing paired t-tests and calculating confidence intervals\n",
    "t_statistic_ann_vs_linear, p_value_ann_vs_linear = ttest_rel(mse_ann, mse_linear)\n",
    "ci_ann_vs_linear = compute_confidence_interval(mse_ann, mse_linear)\n",
    "\n",
    "t_statistic_ann_vs_baseline, p_value_ann_vs_baseline = ttest_rel(mse_ann, mse_baseline)\n",
    "ci_ann_vs_baseline = compute_confidence_interval(mse_ann, mse_baseline)\n",
    "\n",
    "t_statistic_linear_vs_baseline, p_value_linear_vs_baseline = ttest_rel(mse_linear, mse_baseline)\n",
    "ci_linear_vs_baseline = compute_confidence_interval(mse_linear, mse_baseline)\n",
    "\n",
    "# Printing the results\n",
    "print(f\"ANN vs. Linear Regression: t-statistic = {t_statistic_ann_vs_linear}, p-value = {p_value_ann_vs_linear}, CI = {ci_ann_vs_linear}\")\n",
    "print(f\"ANN vs. Baseline: t-statistic = {t_statistic_ann_vs_baseline}, p-value = {p_value_ann_vs_baseline}, CI = {ci_ann_vs_baseline}\")\n",
    "print(f\"Linear Regression vs. Baseline: t-statistic = {t_statistic_linear_vs_baseline}, p-value = {p_value_linear_vs_baseline}, CI = {ci_linear_vs_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "price_quantiles = df['price'].quantile([0.33, 0.66])\n",
    "low_threshold = price_quantiles[0.33]\n",
    "high_threshold = price_quantiles[0.66]\n",
    "\n",
    "# Function to categorize prices\n",
    "def categorize_price(price):\n",
    "    if price <= low_threshold:\n",
    "        return 'low'\n",
    "    elif price <= high_threshold:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "# Apply the function to create a new categorical variable\n",
    "df['price_category'] = df['price'].apply(categorize_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select lambda value for classification, optimal...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define range of lambda values to test\n",
    "lambda_values = np.logspace(-4, 4, 100)\n",
    "C_values = 1 / lambda_values  # Convert lambda values to C values for Logistic Regression\n",
    "\n",
    "all_errors = {}\n",
    "\n",
    "# Initialize list to store generalization errors for each C value\n",
    "generalization_errors = []\n",
    "\n",
    "# Define number of folds for KFold cross-validation\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "fold_number = 0\n",
    "fold_error_list = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Initialize list to store error for each fold\n",
    "    error_list = []\n",
    "\n",
    "    # Loop through each C value\n",
    "    for c in C_values:\n",
    "        # Create Multinomial Logistic Regression model object with given C value\n",
    "        model = LogisticRegression(C=c, max_iter=1000, solver='lbfgs', random_state=42)\n",
    "        \n",
    "        # Fit the model on the training data for this fold\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Make predictions on the testing data for this fold\n",
    "        fold_pred = model.predict(X_fold_test)\n",
    "        \n",
    "        # Calculate the error (1 - accuracy) for this fold\n",
    "        fold_error = 1 - accuracy_score(y_fold_test, fold_pred)\n",
    "        \n",
    "        # Append the error to the error list\n",
    "        error_list.append(fold_error)\n",
    "\n",
    "    fold_error_list.append(error_list)\n",
    "\n",
    "    # Calculate the average error across all folds\n",
    "    generalization_error = np.mean(error_list)\n",
    "\n",
    "    err_dict = {'best_lambda': (lambda_values[error_list.index(min(error_list))]), 'error': min(error_list)}\n",
    "\n",
    "    all_errors[fold_number] = err_dict\n",
    "\n",
    "    # Append the generalization error to the list\n",
    "    generalization_errors.append(generalization_error)\n",
    "\n",
    "    fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrix to store errors for each C value across all folds\n",
    "errors_matrix = np.zeros((len(C_values), n_folds))\n",
    "\n",
    "# Loop through each fold\n",
    "for fold_idx, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_fold_train, X_fold_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Loop through each C value\n",
    "    for c_idx, c in enumerate(C_values):\n",
    "        # Create Logistic Regression model object with given C value\n",
    "        model = LogisticRegression(C=c, max_iter=1000, solver='lbfgs', random_state=42)\n",
    "        \n",
    "        # Fit the model on the training data for this fold\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Make predictions on the testing data for this fold\n",
    "        fold_pred = model.predict(X_fold_test)\n",
    "        \n",
    "        # Calculate the error (1 - accuracy) for this fold\n",
    "        fold_error = 1 - accuracy_score(y_fold_test, fold_pred)\n",
    "        \n",
    "        # Store the error in the matrix\n",
    "        errors_matrix[c_idx, fold_idx] = fold_error\n",
    "\n",
    "# Calculate the average error across all folds for each C value\n",
    "generalization_errors = np.mean(errors_matrix, axis=1)\n",
    "\n",
    "# Plotting the generalization error for different C values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, generalization_errors, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C value (Inverse of Lambda)')\n",
    "plt.ylabel('Average Generalization Error')\n",
    "plt.title('Generalization Error vs. C value in Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for error in all_errors:\n",
    "    print(f\"Fold {error}: {all_errors[error]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = 9\n",
    "\n",
    "print(f\"Minimum Generalization Error: {min(generalization_errors)}\")\n",
    "print(f\"Lambda value with minimum generalization error: {(lambda_values[generalization_errors.index(min(generalization_errors))])}\")\n",
    "\n",
    "#Average generalisation error vs regularisation parameter (lambda)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(lambda_values, fold_error_list[best_fold-1], marker='o')\n",
    "plt.xlabel('λ (Regularization Parameter)')\n",
    "plt.ylabel('Average Generalization Error')\n",
    "plt.title('Generalization Error as a Function of λ')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Lambda values for Logistic Regression\n",
    "lambda_values = np.logspace(-1, 1, 50)\n",
    "C_values = 1 / lambda_values\n",
    "\n",
    "# Define outer and inner CV splits\n",
    "outer_cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=10, shuffle=True, random_state=43)\n",
    "\n",
    "results = []\n",
    "\n",
    "fold_number = 0\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    fold_number += 1\n",
    "    print(f\"Fold {fold_number} running.\")\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Baseline model: predicting the most common class\n",
    "    most_common_class = y_train.value_counts().idxmax()\n",
    "    y_pred_base = [most_common_class] * len(y_test)\n",
    "    error_rate_base = 1 - accuracy_score(y_test, y_pred_base)\n",
    "    \n",
    "    # Logistic Regression with CV for hyperparameter tuning\n",
    "    best_acc_logreg = 0\n",
    "    best_c = None\n",
    "    for c in C_values:\n",
    "        logreg = LogisticRegression(C=c, max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "        inner_accs = []\n",
    "        for inner_train_idx, inner_val_idx in inner_cv.split(X_train, y_train):\n",
    "            X_inner_train, X_val = X_train.iloc[inner_train_idx], X_train.iloc[inner_val_idx]\n",
    "            y_inner_train, y_val = y_train.iloc[inner_train_idx], y_train.iloc[inner_val_idx]\n",
    "            \n",
    "            logreg.fit(X_inner_train, y_inner_train)\n",
    "            y_pred = logreg.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            inner_accs.append(acc)\n",
    "        \n",
    "        avg_acc = np.mean(inner_accs)\n",
    "        if avg_acc > best_acc_logreg:\n",
    "            best_acc_logreg = avg_acc\n",
    "            best_c = c\n",
    "    \n",
    "    # ANN with Randomized Search for hyperparameter tuning\n",
    "    ann = MLPClassifier(max_iter=1000, random_state=42)\n",
    "    param_distributions = {\n",
    "        'hidden_layer_sizes': [(1,), (15,), (30,), (50,)]  # Testing different sizes for a single hidden layer\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(ann, param_distributions, n_iter=4, cv=inner_cv, scoring='accuracy', random_state=44, n_jobs=-1)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_ann = random_search.best_estimator_\n",
    "    optimal_hidden_layer_size = best_ann.get_params()['hidden_layer_sizes']\n",
    "    # Error rate for the best models on the test set\n",
    "    logreg_best = LogisticRegression(C=best_c, max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "    logreg_best.fit(X_train, y_train)\n",
    "    y_pred_logreg = logreg_best.predict(X_test)\n",
    "    \n",
    "    y_pred_ann = best_ann.predict(X_test)\n",
    "    \n",
    "    error_rate_logreg = 1 - accuracy_score(y_test, y_pred_logreg)\n",
    "    error_rate_ann = 1 - accuracy_score(y_test, y_pred_ann)\n",
    "    \n",
    "    results.append((error_rate_base, error_rate_logreg, error_rate_ann, best_c, optimal_hidden_layer_size))\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Baseline Error: {result[0]:.4f}, LogReg Error (C={result[3]}): {result[1]:.4f}, \"\n",
    "          f\"ANN Error (hidden_layer_sizes={result[4]}): {result[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['Outer fold', 'Logistic regression', 'ANN', 'Baseline', 'λ (Logistic Regression)', 'α (ANN)']\n",
    "data = []\n",
    "for i, (base, logreg, ann, c, alpha) in enumerate(results, 1):\n",
    "    data.append([i, logreg, ann, base, 1/c, alpha])\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a contingency table\n",
    "def create_contingency_table(predictions1, predictions2):\n",
    "    return np.array([\n",
    "        sum((predictions1 == y_test) & (predictions2 == y_test)),\n",
    "        sum((predictions1 == y_test) & (predictions2 != y_test)),\n",
    "        sum((predictions1 != y_test) & (predictions2 == y_test)),\n",
    "        sum((predictions1 != y_test) & (predictions2 != y_test))\n",
    "    ]).reshape(2, 2)\n",
    "\n",
    "# Creating contingency tables\n",
    "contingency_table_1 = create_contingency_table(y_pred_logreg, y_pred_ann)\n",
    "contingency_table_2 = create_contingency_table(y_pred_logreg, y_pred_base)\n",
    "contingency_table_3 = create_contingency_table(y_pred_ann, y_pred_base)\n",
    "\n",
    "# Performing McNemar's tests\n",
    "result_1 = mcnemar(contingency_table_1, exact=False)\n",
    "result_2 = mcnemar(contingency_table_2, exact=False)\n",
    "result_3 = mcnemar(contingency_table_3, exact=False)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Logistic Regression vs MLP Classifier:\")\n",
    "print(f\"p-value: {result_1.pvalue}\")\n",
    "\n",
    "print(\"Logistic Regression vs Dummy Classifier:\")\n",
    "print(f\"p-value: {result_2.pvalue}\")\n",
    "\n",
    "print(\"MLP Classifier vs Dummy Classifier:\")\n",
    "print(f\"p-value: {result_3.pvalue}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
